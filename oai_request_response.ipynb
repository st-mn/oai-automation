{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the dependicies\n",
        "Run the code below to install the depencies we need for our functions\n",
        "Define the functions\n",
        "The following code defines the functions we need to construct the index and query it"
      ],
      "metadata": {
        "id": "SAuiMs5n6atU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AKE5afr5uF2"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "import requests\n",
        "import argparse"
      ],
      "metadata": {
        "id": "yiZz-FTA63_2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "api_endpoint = \"https://api.openai.com/v1/completions\"\n",
        "api_key = \"\"\n",
        "\n",
        "request_headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + api_key\n",
        "}\n",
        "\n",
        "request_data = {\n",
        "    \"model\": \"text-davinci-003\",\n",
        "    \"prompt\": f\"Write python script to extract summary of a webpage. Provide only code, no text\",\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.5\n",
        "}\n",
        "\n",
        "response = requests.post(api_endpoint, headers=request_headers, json=request_data)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    response_text = response.json()[\"choices\"][0][\"text\"]\n",
        "    print(response_text)\n",
        "else:\n",
        "    print(f\"Request failed with status code: {str(response.status_code)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr6-Ok4o7Prn",
        "outputId": "a46f3e72-9e72-4c15-9ac1-94aef5ca8a9d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "import requests\n",
            "from bs4 import BeautifulSoup\n",
            "\n",
            "# Provide the URL to be scraped\n",
            "url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
            "\n",
            "# Make a GET request to fetch the raw HTML content\n",
            "html_content = requests.get(url).text\n",
            "\n",
            "# Parse the html content\n",
            "soup = BeautifulSoup(html_content, \"lxml\")\n",
            "\n",
            "# Find the text within the class attribute\n",
            "summary = soup.find(\"div\", attrs={\"class\": \"mw-parser-output\"}).text\n",
            "\n",
            "# Print the scraped text\n",
            "print(summary)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tCPXXRbC6f-h"
      }
    }
  ]
}